{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.3 (SDL 2.0.22, Python 3.11.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scene import Scene\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "\n",
    "scene = Scene(using_cnn=False, init_randomly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = scene.feature_count\n",
    "model = keras.Sequential([\n",
    "    Dense(input_shape, activation='relu', kernel_initializer='he_normal', input_shape=(input_shape,)),\n",
    "    Dense(4, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "target = keras.models.clone_model(model)\n",
    "target.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(4)\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis], verbose=False)\n",
    "        return np.argmax(Q_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# (state, action, reward, next_state, done)\n",
    "replay_memory = deque(maxlen=2000)\n",
    "\n",
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_memory), size=batch_size)\n",
    "    batch = [replay_memory[index] for index in indices]\n",
    "    states, actions, rewards, next_states, dones = [np.array([experience[field_index]\n",
    "                                                            for experience in batch])\n",
    "                                                            for field_index in range(5)]\n",
    "    return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(scene, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    scene.snake.change_direction(action)\n",
    "    next_state, reward, done = scene.move()\n",
    "    replay_memory.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_rate = 0.95\n",
    "optimizer = keras.optimizers.legacy.Adam(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "def training_step(batch_size):\n",
    "    states, actions, rewards, next_states, dones = sample_experiences(batch_size)\n",
    "    next_Q_values = model.predict(next_states, verbose=False)\n",
    "    best_next_actions = np.argmax(next_Q_values, axis=1)\n",
    "    next_mask = tf.one_hot(best_next_actions, 4).numpy()\n",
    "    next_best_Q_values = (target.predict(next_states, verbose=False) * next_mask).sum(axis=1)\n",
    "    target_Q_values = (rewards + (1 - dones) * discount_rate * next_best_Q_values).reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, 4)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode number:  100\n",
      "Average reward:  -62.06\n",
      "Episode number:  150\n",
      "Average reward:  -30.4\n",
      "Episode number:  200\n",
      "Average reward:  -30.44\n",
      "Episode number:  250\n",
      "Average reward:  -29.44\n",
      "Episode number:  300\n",
      "Average reward:  -30.44\n",
      "Episode number:  350\n",
      "Average reward:  -29.14\n",
      "Episode number:  400\n",
      "Average reward:  -31.5\n",
      "Episode number:  450\n",
      "Average reward:  -30.98\n",
      "Episode number:  500\n",
      "Average reward:  -29.2\n",
      "Episode number:  550\n",
      "Average reward:  -29.46\n",
      "Episode number:  600\n",
      "Average reward:  -29.9\n",
      "Episode number:  650\n",
      "Average reward:  -27.88\n",
      "Episode number:  700\n",
      "Average reward:  -25.06\n",
      "Episode number:  750\n",
      "Average reward:  -27.46\n",
      "Episode number:  800\n",
      "Average reward:  -26.02\n",
      "Episode number:  850\n",
      "Average reward:  -26.78\n",
      "Episode number:  900\n",
      "Average reward:  -24.88\n",
      "Episode number:  950\n",
      "Average reward:  -25.24\n",
      "Episode number:  1000\n",
      "Average reward:  -24.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juanpablo/miniconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode number:  1050\n",
      "Average reward:  -21.7\n",
      "Episode number:  1100\n",
      "Average reward:  -25.06\n",
      "Episode number:  1150\n",
      "Average reward:  -21.78\n",
      "Episode number:  1200\n",
      "Average reward:  -24.24\n",
      "Episode number:  1250\n",
      "Average reward:  -21.62\n",
      "Episode number:  1300\n",
      "Average reward:  -23.86\n",
      "Episode number:  1350\n",
      "Average reward:  -18.06\n",
      "Episode number:  1400\n",
      "Average reward:  -19.86\n",
      "Episode number:  1450\n",
      "Average reward:  -16.12\n",
      "Episode number:  1500\n",
      "Average reward:  -16.58\n",
      "Episode number:  1550\n",
      "Average reward:  -17.4\n",
      "Episode number:  1600\n",
      "Average reward:  -20.06\n",
      "Episode number:  1650\n",
      "Average reward:  -17.58\n",
      "Episode number:  1700\n",
      "Average reward:  -22.54\n",
      "Episode number:  1750\n",
      "Average reward:  -17.62\n",
      "Episode number:  1800\n",
      "Average reward:  -20.0\n",
      "Episode number:  1850\n",
      "Average reward:  -16.06\n",
      "Episode number:  1900\n",
      "Average reward:  -14.64\n",
      "Episode number:  1950\n",
      "Average reward:  -18.6\n",
      "Episode number:  2000\n",
      "Average reward:  -16.8\n",
      "Episode number:  2050\n",
      "Average reward:  -16.78\n",
      "Episode number:  2100\n",
      "Average reward:  -10.96\n",
      "Episode number:  2150\n",
      "Average reward:  -13.6\n",
      "Episode number:  2200\n",
      "Average reward:  -12.14\n",
      "Episode number:  2250\n",
      "Average reward:  -12.96\n",
      "Episode number:  2300\n",
      "Average reward:  -13.66\n",
      "Episode number:  2350\n",
      "Average reward:  -8.56\n",
      "Episode number:  2400\n",
      "Average reward:  -16.22\n",
      "Episode number:  2450\n",
      "Average reward:  -10.0\n",
      "Episode number:  2500\n",
      "Average reward:  -15.1\n",
      "Episode number:  2550\n",
      "Average reward:  -20.54\n",
      "Episode number:  2600\n",
      "Average reward:  -12.34\n",
      "Episode number:  2650\n",
      "Average reward:  -8.94\n",
      "Episode number:  2700\n",
      "Average reward:  -8.44\n",
      "Episode number:  2750\n",
      "Average reward:  -7.44\n",
      "Episode number:  2800\n",
      "Average reward:  -13.4\n",
      "Episode number:  2850\n",
      "Average reward:  -10.42\n",
      "Episode number:  2900\n",
      "Average reward:  -8.28\n",
      "Episode number:  2950\n",
      "Average reward:  -9.86\n",
      "Episode number:  3000\n",
      "Average reward:  -8.6\n",
      "Episode number:  3050\n",
      "Average reward:  -16.66\n",
      "Episode number:  3100\n",
      "Average reward:  -16.36\n",
      "Episode number:  3150\n",
      "Average reward:  -14.12\n",
      "Episode number:  3200\n",
      "Average reward:  -12.44\n",
      "Episode number:  3250\n",
      "Average reward:  -0.38\n",
      "Episode number:  3300\n",
      "Average reward:  -11.98\n",
      "Episode number:  3350\n",
      "Average reward:  -11.04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/juanpablo/Desktop/Projects/Snake/model_using_features.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_features.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_features.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_features.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     state, reward, done \u001b[39m=\u001b[39m play_one_step(scene, state, epsilon)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_features.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     reward_per_batch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_features.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m episode \u001b[39m>\u001b[39m \u001b[39m50\u001b[39m:\n",
      "\u001b[1;32m/Users/juanpablo/Desktop/Projects/Snake/model_using_features.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_features.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplay_one_step\u001b[39m(scene, state, epsilon):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_features.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     action \u001b[39m=\u001b[39m epsilon_greedy_policy(state, epsilon)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_features.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     scene\u001b[39m.\u001b[39msnake\u001b[39m.\u001b[39mchange_direction(action)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_features.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     next_state, reward, done \u001b[39m=\u001b[39m scene\u001b[39m.\u001b[39mmove()\n",
      "\u001b[1;32m/Users/juanpablo/Desktop/Projects/Snake/model_using_features.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_features.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m4\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_features.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_features.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     Q_values \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(state[np\u001b[39m.\u001b[39;49mnewaxis], verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_features.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39margmax(Q_values[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/keras/src/engine/training.py:2521\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2512\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m   2513\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   2514\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2515\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2518\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   2519\u001b[0m         )\n\u001b[0;32m-> 2521\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[1;32m   2522\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m   2523\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   2524\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[1;32m   2525\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m   2526\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m   2527\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   2528\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   2529\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   2530\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2531\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution,\n\u001b[1;32m   2532\u001b[0m )\n\u001b[1;32m   2534\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   2535\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1678\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorExactEvalDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1677\u001b[0m     \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 1678\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1285\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1282\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[1;32m   1284\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1285\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[1;32m   1286\u001b[0m     x,\n\u001b[1;32m   1287\u001b[0m     y,\n\u001b[1;32m   1288\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1289\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m   1290\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[1;32m   1291\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1292\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   1293\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1294\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1295\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1296\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[1;32m   1297\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1298\u001b[0m     pss_evaluation_shards\u001b[39m=\u001b[39;49mpss_evaluation_shards,\n\u001b[1;32m   1299\u001b[0m )\n\u001b[1;32m   1301\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[1;32m   1303\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:314\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[39mreturn\u001b[39;00m indices\n\u001b[1;32m    309\u001b[0m \u001b[39m# We prefetch a single element. Computing large permutations can take\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39m# quite a while so we don't want to wait for prefetching over an epoch\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[39m# boundary to trigger the next permutation. On the other hand, too many\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39m# simultaneous shuffles can contend on a hardware level and degrade all\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m# performance.\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39;49mmap(permutation)\u001b[39m.\u001b[39mprefetch(\u001b[39m1\u001b[39m)\n\u001b[1;32m    316\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mslice_batch_indices\u001b[39m(indices):\n\u001b[1;32m    317\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Convert a Tensor of indices into a dataset of batched indices.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \n\u001b[1;32m    319\u001b[0m \u001b[39m    This step can be accomplished in several ways. The most natural is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[39m      A Dataset of batched indices.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:2278\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2274\u001b[0m \u001b[39m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[1;32m   2275\u001b[0m \u001b[39m# dataset_ops).\u001b[39;00m\n\u001b[1;32m   2276\u001b[0m \u001b[39m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m   2277\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m map_op\n\u001b[0;32m-> 2278\u001b[0m \u001b[39mreturn\u001b[39;00m map_op\u001b[39m.\u001b[39;49m_map_v2(\n\u001b[1;32m   2279\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2280\u001b[0m     map_func,\n\u001b[1;32m   2281\u001b[0m     num_parallel_calls\u001b[39m=\u001b[39;49mnum_parallel_calls,\n\u001b[1;32m   2282\u001b[0m     deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[1;32m   2283\u001b[0m     name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/map_op.py:37\u001b[0m, in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[39mif\u001b[39;00m deterministic \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m debug_mode\u001b[39m.\u001b[39mDEBUG_MODE:\n\u001b[1;32m     35\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39m`num_parallel_calls` argument is specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m   \u001b[39mreturn\u001b[39;00m _MapDataset(\n\u001b[1;32m     38\u001b[0m       input_dataset, map_func, preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m   \u001b[39mreturn\u001b[39;00m _ParallelMapDataset(\n\u001b[1;32m     41\u001b[0m       input_dataset,\n\u001b[1;32m     42\u001b[0m       map_func,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m       preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     46\u001b[0m       name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/map_op.py:113\u001b[0m, in \u001b[0;36m_MapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m structured_function\u001b[39m.\u001b[39mStructuredFunctionWrapper(\n\u001b[1;32m    108\u001b[0m     map_func,\n\u001b[1;32m    109\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transformation_name(),\n\u001b[1;32m    110\u001b[0m     dataset\u001b[39m=\u001b[39minput_dataset,\n\u001b[1;32m    111\u001b[0m     use_legacy_function\u001b[39m=\u001b[39muse_legacy_function)\n\u001b[1;32m    112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m name\n\u001b[0;32m--> 113\u001b[0m variant_tensor \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49mmap_dataset(\n\u001b[1;32m    114\u001b[0m     input_dataset\u001b[39m.\u001b[39;49m_variant_tensor,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_func\u001b[39m.\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs,\n\u001b[1;32m    116\u001b[0m     f\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_func\u001b[39m.\u001b[39;49mfunction,\n\u001b[1;32m    117\u001b[0m     use_inter_op_parallelism\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_use_inter_op_parallelism,\n\u001b[1;32m    118\u001b[0m     preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preserve_cardinality,\n\u001b[1;32m    119\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_common_args)\n\u001b[1;32m    120\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3471\u001b[0m, in \u001b[0;36mmap_dataset\u001b[0;34m(input_dataset, other_arguments, f, output_types, output_shapes, use_inter_op_parallelism, preserve_cardinality, metadata, name)\u001b[0m\n\u001b[1;32m   3469\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   3470\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3471\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   3472\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMapDataset\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, input_dataset, other_arguments, \u001b[39m\"\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m, f,\n\u001b[1;32m   3473\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39moutput_types\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_types, \u001b[39m\"\u001b[39;49m\u001b[39moutput_shapes\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_shapes,\n\u001b[1;32m   3474\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39muse_inter_op_parallelism\u001b[39;49m\u001b[39m\"\u001b[39;49m, use_inter_op_parallelism,\n\u001b[1;32m   3475\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mpreserve_cardinality\u001b[39;49m\u001b[39m\"\u001b[39;49m, preserve_cardinality, \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m, metadata)\n\u001b[1;32m   3476\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   3477\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episodes_count = 8000\n",
    "reward_per_batch = 0\n",
    "\n",
    "for episode in range(1, episodes_count + 1):\n",
    "    state = scene.scene_as_feature_vector()\n",
    "    epsilon = (1 / (np.linspace(1, 8, episodes_count)**(1/2)))[episode - 1]\n",
    "    done = False\n",
    "    \n",
    "    steps = 0\n",
    "    while not done:\n",
    "        steps += 1\n",
    "        state, reward, done = play_one_step(scene, state, epsilon)\n",
    "        reward_per_batch += reward\n",
    "        \n",
    "    if episode > 50:\n",
    "        training_step(batch_size)\n",
    "        if episode % 50 == 0:\n",
    "            print(\"Episode number: \", episode)\n",
    "            target.set_weights(model.get_weights())\n",
    "            print(\"Average reward: \", reward_per_batch / 50)\n",
    "            reward_per_batch = 0\n",
    "            \n",
    "    if episode % 1000 == 0:\n",
    "        model.save(f\"models/features/{episode}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
