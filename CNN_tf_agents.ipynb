{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "import tf_agents.networks.network as network\n",
    "from tf_agents.specs import tensor_spec\n",
    "from snake_game import SnakeGame\n",
    "from scene import Scene\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Input\n",
    "\n",
    "scene = Scene(init_randomly=True)\n",
    "episodes_count = 50000\n",
    "random_episodes = 20000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "\n",
    "env = SnakeGame(scene)\n",
    "env = TFPyEnvironment(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyQNetwork(network.Network):\n",
    "    def __init__(self,\n",
    "                 input_tensor_spec,\n",
    "                 action_spec,\n",
    "                 name='MyQNetwork'):\n",
    "        super(MyQNetwork, self).__init__(\n",
    "            input_tensor_spec=input_tensor_spec,\n",
    "            state_spec=(),\n",
    "            name=name)\n",
    "        \n",
    "        self._action_spec = action_spec\n",
    "\n",
    "        input_shape = input_tensor_spec.shape\n",
    "        \n",
    "        self._model = Sequential([\n",
    "            Input(shape=input_shape),\n",
    "            Conv2D(32, (2, 2), 1, activation='relu', kernel_initializer='he_normal'),\n",
    "            Conv2D(64, (2, 2), 1, activation='relu', kernel_initializer='he_normal'),\n",
    "            Flatten(),\n",
    "            Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "            Dense(16, activation='relu', kernel_initializer='he_normal'),\n",
    "            Dense(4, activation='linear')\n",
    "        ])\n",
    "\n",
    "    def call(self, observations, step_type=None, network_state=(), training=False):\n",
    "        output = self._model(observations)\n",
    "        return output, network_state\n",
    "\n",
    "q_net = MyQNetwork(\n",
    "    env.observation_spec(),\n",
    "    env.action_spec())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# Create the agent\n",
    "from tf_agents.agents.dqn.dqn_agent import DdqnAgent\n",
    "from tensorflow import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import Huber\n",
    "from tf_agents.trajectories import TimeStep\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "optimizer = Adam(learning_rate=0.003)\n",
    "\n",
    "# epsilon = lambda train_step: (1 / (tf.cast(tf.linspace(1, 8, episodes_count)**3, tf.float32)))[train_step]\n",
    "epsilon = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=1.0,\n",
    "    decay_steps=episodes_count,\n",
    "    end_learning_rate=0.01\n",
    ")\n",
    "\n",
    "agent = DdqnAgent(\n",
    "    time_step_spec=env.time_step_spec(),\n",
    "    action_spec=env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=Huber(reduction=\"none\"),\n",
    "    gamma=tf.constant(0.95, dtype=tf.float32),\n",
    "    train_step_counter=train_step_counter,\n",
    "    epsilon_greedy=lambda: epsilon(train_step_counter)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the replay buffer\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=env.batch_size,\n",
    "    max_length=100000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training metrics\n",
    "from tf_agents.metrics import tf_metrics\n",
    "\n",
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(),\n",
    "    tf_metrics.AverageEpisodeLengthMetric()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the driver\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "\n",
    "collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    env,\n",
    "    policy=agent.collect_policy,\n",
    "    observers=[replay_buffer.add_batch] + train_metrics,\n",
    "    num_steps=4\n",
    ")\n",
    "\n",
    "# Run a random policy to fill the replay buffer\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "\n",
    "initial_collect_policy = RandomTFPolicy(env.time_step_spec(), env.action_spec())\n",
    "init_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    env,\n",
    "    policy=initial_collect_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=random_episodes\n",
    ")\n",
    "\n",
    "final_time_step, final_policy_state = init_driver.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/juanpablo/miniconda3/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/juanpablo/miniconda3/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=64,\n",
    "    num_steps=2, # Capaz cambiar esto\n",
    "    num_parallel_calls=3\n",
    ").prefetch(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training loop\n",
    "\n",
    "from tf_agents.utils.common import function\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.eval.metric_utils import log_metrics\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "collect_driver.run = function(collect_driver.run)\n",
    "agent.train = function(agent.train)\n",
    "\n",
    "def train_agent(n_iterations):\n",
    "    time_step = None\n",
    "    policy_state = agent.collect_policy.get_initial_state(env.batch_size)\n",
    "    iterator = iter(dataset)\n",
    "    for iteration in range(n_iterations):\n",
    "        time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
    "        trajectories, buffer_info = next(iterator)\n",
    "        train_loss = agent.train(trajectories)\n",
    "        if iteration % 1000 == 0:\n",
    "            print(\"Iteration: \", iteration)\n",
    "            log_metrics(train_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/juanpablo/miniconda3/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/juanpablo/miniconda3/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 3\n",
      "\t\t EnvironmentSteps = 4\n",
      "\t\t AverageReturn = -31.66666603088379\n",
      "\t\t AverageEpisodeLength = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 2752\n",
      "\t\t EnvironmentSteps = 4004\n",
      "\t\t AverageReturn = -40109.1015625\n",
      "\t\t AverageEpisodeLength = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 5533\n",
      "\t\t EnvironmentSteps = 8004\n",
      "\t\t AverageReturn = -80904.3984375\n",
      "\t\t AverageEpisodeLength = 2.0999999046325684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 8275\n",
      "\t\t EnvironmentSteps = 12004\n",
      "\t\t AverageReturn = -121046.1015625\n",
      "\t\t AverageEpisodeLength = 1.100000023841858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 10878\n",
      "\t\t EnvironmentSteps = 16004\n",
      "\t\t AverageReturn = -158638.90625\n",
      "\t\t AverageEpisodeLength = 0.800000011920929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 13364\n",
      "\t\t EnvironmentSteps = 20004\n",
      "\t\t AverageReturn = -194800.09375\n",
      "\t\t AverageEpisodeLength = 3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 15758\n",
      "\t\t EnvironmentSteps = 24004\n",
      "\t\t AverageReturn = -228973.296875\n",
      "\t\t AverageEpisodeLength = 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 17934\n",
      "\t\t EnvironmentSteps = 28004\n",
      "\t\t AverageReturn = -259571.5\n",
      "\t\t AverageEpisodeLength = 4.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 20059\n",
      "\t\t EnvironmentSteps = 32004\n",
      "\t\t AverageReturn = -288391.90625\n",
      "\t\t AverageEpisodeLength = 2.700000047683716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 22130\n",
      "\t\t EnvironmentSteps = 36004\n",
      "\t\t AverageReturn = -316464.09375\n",
      "\t\t AverageEpisodeLength = 2.4000000953674316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 24019\n",
      "\t\t EnvironmentSteps = 40004\n",
      "\t\t AverageReturn = -341380.0\n",
      "\t\t AverageEpisodeLength = 5.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 25966\n",
      "\t\t EnvironmentSteps = 44004\n",
      "\t\t AverageReturn = -366688.59375\n",
      "\t\t AverageEpisodeLength = 2.4000000953674316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  11000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 27900\n",
      "\t\t EnvironmentSteps = 48004\n",
      "\t\t AverageReturn = -391426.1875\n",
      "\t\t AverageEpisodeLength = 2.5999999046325684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 29678\n",
      "\t\t EnvironmentSteps = 52004\n",
      "\t\t AverageReturn = -412885.59375\n",
      "\t\t AverageEpisodeLength = 1.399999976158142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  13000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 31417\n",
      "\t\t EnvironmentSteps = 56004\n",
      "\t\t AverageReturn = -434034.5\n",
      "\t\t AverageEpisodeLength = 1.2999999523162842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  14000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 33133\n",
      "\t\t EnvironmentSteps = 60004\n",
      "\t\t AverageReturn = -453831.1875\n",
      "\t\t AverageEpisodeLength = 2.4000000953674316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  15000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 34675\n",
      "\t\t EnvironmentSteps = 64004\n",
      "\t\t AverageReturn = -470226.0\n",
      "\t\t AverageEpisodeLength = 1.399999976158142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 36348\n",
      "\t\t EnvironmentSteps = 68004\n",
      "\t\t AverageReturn = -488173.3125\n",
      "\t\t AverageEpisodeLength = 2.0999999046325684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  17000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 37781\n",
      "\t\t EnvironmentSteps = 72004\n",
      "\t\t AverageReturn = -501976.40625\n",
      "\t\t AverageEpisodeLength = 3.9000000953674316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  18000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 39137\n",
      "\t\t EnvironmentSteps = 76004\n",
      "\t\t AverageReturn = -513631.3125\n",
      "\t\t AverageEpisodeLength = 3.799999952316284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  19000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 40461\n",
      "\t\t EnvironmentSteps = 80004\n",
      "\t\t AverageReturn = -525122.0\n",
      "\t\t AverageEpisodeLength = 2.200000047683716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 41723\n",
      "\t\t EnvironmentSteps = 84004\n",
      "\t\t AverageReturn = -535672.8125\n",
      "\t\t AverageEpisodeLength = 4.900000095367432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  21000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 42929\n",
      "\t\t EnvironmentSteps = 88004\n",
      "\t\t AverageReturn = -544278.0\n",
      "\t\t AverageEpisodeLength = 2.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  22000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 44116\n",
      "\t\t EnvironmentSteps = 92004\n",
      "\t\t AverageReturn = -553169.8125\n",
      "\t\t AverageEpisodeLength = 3.200000047683716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  23000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 45191\n",
      "\t\t EnvironmentSteps = 96004\n",
      "\t\t AverageReturn = -558899.125\n",
      "\t\t AverageEpisodeLength = 2.700000047683716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  24000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 46294\n",
      "\t\t EnvironmentSteps = 100004\n",
      "\t\t AverageReturn = -563664.125\n",
      "\t\t AverageEpisodeLength = 3.4000000953674316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 47304\n",
      "\t\t EnvironmentSteps = 104004\n",
      "\t\t AverageReturn = -567380.0\n",
      "\t\t AverageEpisodeLength = 4.599999904632568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  26000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 48228\n",
      "\t\t EnvironmentSteps = 108004\n",
      "\t\t AverageReturn = -569308.375\n",
      "\t\t AverageEpisodeLength = 5.099999904632568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  27000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 49168\n",
      "\t\t EnvironmentSteps = 112004\n",
      "\t\t AverageReturn = -569971.625\n",
      "\t\t AverageEpisodeLength = 3.0999999046325684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  28000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 50083\n",
      "\t\t EnvironmentSteps = 116004\n",
      "\t\t AverageReturn = -570232.0\n",
      "\t\t AverageEpisodeLength = 6.400000095367432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  29000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 51026\n",
      "\t\t EnvironmentSteps = 120004\n",
      "\t\t AverageReturn = -570091.375\n",
      "\t\t AverageEpisodeLength = 6.099999904632568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 51868\n",
      "\t\t EnvironmentSteps = 124004\n",
      "\t\t AverageReturn = -568590.875\n",
      "\t\t AverageEpisodeLength = 5.900000095367432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  31000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 52622\n",
      "\t\t EnvironmentSteps = 128004\n",
      "\t\t AverageReturn = -565556.625\n",
      "\t\t AverageEpisodeLength = 5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  32000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 53333\n",
      "\t\t EnvironmentSteps = 132004\n",
      "\t\t AverageReturn = -560909.125\n",
      "\t\t AverageEpisodeLength = 8.300000190734863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  33000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 54056\n",
      "\t\t EnvironmentSteps = 136004\n",
      "\t\t AverageReturn = -555503.125\n",
      "\t\t AverageEpisodeLength = 5.300000190734863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  34000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 54726\n",
      "\t\t EnvironmentSteps = 140004\n",
      "\t\t AverageReturn = -548867.0\n",
      "\t\t AverageEpisodeLength = 7.199999809265137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  35000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 55330\n",
      "\t\t EnvironmentSteps = 144004\n",
      "\t\t AverageReturn = -540113.5\n",
      "\t\t AverageEpisodeLength = 5.199999809265137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  36000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 55888\n",
      "\t\t EnvironmentSteps = 148004\n",
      "\t\t AverageReturn = -531148.6875\n",
      "\t\t AverageEpisodeLength = 6.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  37000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 56460\n",
      "\t\t EnvironmentSteps = 152004\n",
      "\t\t AverageReturn = -521412.09375\n",
      "\t\t AverageEpisodeLength = 9.699999809265137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  38000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 56978\n",
      "\t\t EnvironmentSteps = 156004\n",
      "\t\t AverageReturn = -510390.40625\n",
      "\t\t AverageEpisodeLength = 7.099999904632568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  39000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 57453\n",
      "\t\t EnvironmentSteps = 160004\n",
      "\t\t AverageReturn = -497757.5\n",
      "\t\t AverageEpisodeLength = 8.100000381469727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 57884\n",
      "\t\t EnvironmentSteps = 164004\n",
      "\t\t AverageReturn = -484058.1875\n",
      "\t\t AverageEpisodeLength = 6.699999809265137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  41000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 58253\n",
      "\t\t EnvironmentSteps = 168004\n",
      "\t\t AverageReturn = -468653.8125\n",
      "\t\t AverageEpisodeLength = 15.800000190734863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  42000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 58659\n",
      "\t\t EnvironmentSteps = 172004\n",
      "\t\t AverageReturn = -452686.3125\n",
      "\t\t AverageEpisodeLength = 11.899999618530273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  43000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 59046\n",
      "\t\t EnvironmentSteps = 176004\n",
      "\t\t AverageReturn = -436541.90625\n",
      "\t\t AverageEpisodeLength = 9.100000381469727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  44000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 59385\n",
      "\t\t EnvironmentSteps = 180004\n",
      "\t\t AverageReturn = -419169.40625\n",
      "\t\t AverageEpisodeLength = 15.399999618530273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  45000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 59708\n",
      "\t\t EnvironmentSteps = 184004\n",
      "\t\t AverageReturn = -400627.1875\n",
      "\t\t AverageEpisodeLength = 6.300000190734863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  46000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 59999\n",
      "\t\t EnvironmentSteps = 188004\n",
      "\t\t AverageReturn = -381125.0\n",
      "\t\t AverageEpisodeLength = 11.699999809265137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  47000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 60249\n",
      "\t\t EnvironmentSteps = 192004\n",
      "\t\t AverageReturn = -360503.40625\n",
      "\t\t AverageEpisodeLength = 11.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  48000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 60481\n",
      "\t\t EnvironmentSteps = 196004\n",
      "\t\t AverageReturn = -338690.1875\n",
      "\t\t AverageEpisodeLength = 15.899999618530273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  49000\n"
     ]
    }
   ],
   "source": [
    "train_agent(episodes_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 0 0 3 0]\n",
      "  [0 0 0 0 0]\n",
      "  [1 2 0 0 0]\n",
      "  [0 0 0 0 0]]]\n",
      "Action:  [1]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 0 0 3 0]\n",
      "  [1 0 0 0 0]\n",
      "  [2 0 0 0 0]\n",
      "  [0 0 0 0 0]]]\n",
      "Action:  [2]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 0 0 3 0]\n",
      "  [2 1 0 0 0]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 0 0 0]]]\n",
      "Action:  [1]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 1 0 3 0]\n",
      "  [0 2 0 0 0]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 0 0 0]]]\n",
      "Action:  [2]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 2 1 3 0]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 0 0 0]]]\n",
      "Action:  [2]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 2 2 1 0]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 3 0 0]]]\n",
      "Action:  [3]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 0 2 2 0]\n",
      "  [0 0 0 1 0]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 3 0 0]]]\n",
      "Action:  [3]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 0 0 2 0]\n",
      "  [0 0 0 2 0]\n",
      "  [0 0 0 1 0]\n",
      "  [0 0 3 0 0]]]\n",
      "Action:  [3]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 0 2 0]\n",
      "  [0 0 0 2 0]\n",
      "  [0 0 3 1 0]]]\n",
      "Action:  [0]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 0 2 0]\n",
      "  [3 0 0 2 0]\n",
      "  [0 0 1 2 0]]]\n",
      "Action:  [0]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 0 0 0]\n",
      "  [3 0 0 2 0]\n",
      "  [0 1 2 2 0]]]\n",
      "Action:  [0]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 0 0 0]\n",
      "  [3 0 0 0 0]\n",
      "  [1 2 2 2 0]]]\n",
      "Action:  [1]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 3 0 0 0]\n",
      "  [0 0 0 0 0]\n",
      "  [1 0 0 0 0]\n",
      "  [2 2 2 2 0]]]\n",
      "Action:  [1]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 3 0 0 0]\n",
      "  [1 0 0 0 0]\n",
      "  [2 0 0 0 0]\n",
      "  [2 2 2 0 0]]]\n",
      "Action:  [1]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [1 3 0 0 0]\n",
      "  [2 0 0 0 0]\n",
      "  [2 0 0 0 0]\n",
      "  [2 2 0 0 0]]]\n",
      "Action:  [2]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [2 1 0 0 0]\n",
      "  [2 0 0 0 0]\n",
      "  [2 0 0 3 0]\n",
      "  [2 2 0 0 0]]]\n",
      "Action:  [3]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [2 2 0 0 0]\n",
      "  [2 1 0 0 0]\n",
      "  [2 0 0 3 0]\n",
      "  [2 0 0 0 0]]]\n",
      "Action:  [3]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [2 2 0 0 0]\n",
      "  [2 2 0 0 0]\n",
      "  [2 1 0 3 0]\n",
      "  [0 0 0 0 0]]]\n",
      "Action:  [2]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [2 2 0 0 0]\n",
      "  [2 2 0 0 0]\n",
      "  [0 2 1 3 0]\n",
      "  [0 0 0 0 0]]]\n",
      "Action:  [2]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [2 2 0 0 0]\n",
      "  [2 2 0 3 0]\n",
      "  [0 2 2 1 0]\n",
      "  [0 0 0 0 0]]]\n",
      "Action:  [1]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [2 2 0 0 3]\n",
      "  [2 2 0 1 0]\n",
      "  [0 2 2 2 0]\n",
      "  [0 0 0 0 0]]]\n",
      "Action:  [2]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [2 2 0 0 3]\n",
      "  [0 2 0 2 1]\n",
      "  [0 2 2 2 0]\n",
      "  [0 0 0 0 0]]]\n",
      "Action:  [1]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [2 2 0 3 1]\n",
      "  [0 2 0 2 2]\n",
      "  [0 2 2 2 0]\n",
      "  [0 0 0 0 0]]]\n",
      "Action:  [0]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [2 2 0 1 2]\n",
      "  [0 2 0 2 2]\n",
      "  [0 2 2 2 0]\n",
      "  [0 3 0 0 0]]]\n",
      "Action:  [0]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 2 1 2 2]\n",
      "  [0 2 0 2 2]\n",
      "  [0 2 2 2 0]\n",
      "  [0 3 0 0 0]]]\n",
      "Action:  [3]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 0 2 2 2]\n",
      "  [0 2 1 2 2]\n",
      "  [0 2 2 2 0]\n",
      "  [0 3 0 0 0]]]\n",
      "Action:  [0]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 0 2 2 2]\n",
      "  [0 1 2 2 2]\n",
      "  [0 2 2 2 0]\n",
      "  [0 3 0 0 0]]]\n",
      "Action:  [0]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 0 2 2 2]\n",
      "  [1 2 2 2 2]\n",
      "  [0 0 2 2 0]\n",
      "  [0 3 0 0 0]]]\n",
      "Action:  [3]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 0 2 2 2]\n",
      "  [2 2 2 2 2]\n",
      "  [1 0 0 2 0]\n",
      "  [0 3 0 0 0]]]\n",
      "Action:  [3]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 0 2 2 2]\n",
      "  [2 2 2 2 2]\n",
      "  [2 0 0 0 0]\n",
      "  [1 3 0 0 0]]]\n",
      "Action:  [2]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 0 2 2 2]\n",
      "  [2 2 2 2 2]\n",
      "  [2 0 0 3 0]\n",
      "  [2 1 0 0 0]]]\n",
      "Action:  [2]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 0 2 2 2]\n",
      "  [2 2 2 0 2]\n",
      "  [2 0 0 3 0]\n",
      "  [2 2 1 0 0]]]\n",
      "Action:  [2]\n",
      "Map:\n",
      " [[[0 0 0 0 0]\n",
      "  [0 0 2 2 2]\n",
      "  [2 2 2 0 0]\n",
      "  [2 0 0 3 0]\n",
      "  [2 2 2 1 0]]]\n",
      "Action:  [1]\n",
      "Map:\n",
      " [[[3 0 0 0 0]\n",
      "  [0 0 2 2 2]\n",
      "  [2 2 2 0 0]\n",
      "  [2 0 0 1 0]\n",
      "  [2 2 2 2 0]]]\n",
      "Action:  [0]\n",
      "Map:\n",
      " [[[3 0 0 0 0]\n",
      "  [0 0 2 2 0]\n",
      "  [2 2 2 0 0]\n",
      "  [2 0 1 2 0]\n",
      "  [2 2 2 2 0]]]\n",
      "Action:  [0]\n",
      "Map:\n",
      " [[[3 0 0 0 0]\n",
      "  [0 0 2 0 0]\n",
      "  [2 2 2 0 0]\n",
      "  [2 1 2 2 0]\n",
      "  [2 2 2 2 0]]]\n",
      "Action:  [0]\n",
      "Total reward:  210.0\n",
      "Total steps:  36\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the agent\n",
    "\n",
    "time_step = env.reset()\n",
    "rewards = []\n",
    "steps = 0\n",
    "\n",
    "while not time_step.is_last() and steps < 200:\n",
    "    steps += 1\n",
    "    print(\"Map:\\n\", np.argmax(time_step.observation.numpy(), axis=3))\n",
    "    action_step = agent.policy.action(time_step)\n",
    "    print(\"Action: \", action_step.action.numpy())\n",
    "    time_step = env.step(action_step.action)\n",
    "    rewards.append(time_step.reward.numpy()[0])\n",
    "    \n",
    "print(\"Total reward: \", sum(rewards))\n",
    "print(\"Total steps: \", steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0\n",
      "Episode:  10\n",
      "Episode:  20\n",
      "Episode:  30\n",
      "Episode:  40\n",
      "Episode:  50\n",
      "Episode:  60\n",
      "Episode:  70\n",
      "Episode:  80\n",
      "Episode:  90\n",
      "Episode:  100\n",
      "Episode:  110\n",
      "Episode:  120\n",
      "Episode:  130\n",
      "Episode:  140\n",
      "Episode:  150\n",
      "Episode:  160\n",
      "Episode:  170\n",
      "Episode:  180\n",
      "Episode:  190\n",
      "Average steps per episode:  23.31\n",
      "Average reward per episode:  130.65\n",
      "Total 500 episodes:  0\n"
     ]
    }
   ],
   "source": [
    "# Count the average steps per episode\n",
    "\n",
    "n = 200\n",
    "total_steps = 0\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "total_500 = 0\n",
    "\n",
    "for i in range(n):\n",
    "    if i % 10 == 0:\n",
    "        print(\"Episode: \", i)\n",
    "        \n",
    "    time_step = env.reset()\n",
    "    steps = 0\n",
    "    while not time_step.is_last() and steps < 500:\n",
    "        steps += 1\n",
    "        action_step = agent.policy.action(time_step)\n",
    "        time_step = env.step(action_step.action)\n",
    "        reward = time_step.reward.numpy()[0]\n",
    "        total_reward += reward\n",
    "    \n",
    "    if steps == 500:\n",
    "        total_500 += 1\n",
    "    else:\n",
    "        total_steps += steps\n",
    "        \n",
    "if steps == 500:\n",
    "    print(\"500 episodes reached\")\n",
    "    \n",
    "print(\"Average steps per episode: \", total_steps / (n - total_500))\n",
    "print(\"Average reward per episode: \", total_reward / n)\n",
    "print(\"Total 500 episodes: \", total_500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "this __dict__ descriptor does not support '_DictWrapper' objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_agents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicies\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy_saver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PolicySaver\n\u001b[0;32m----> 5\u001b[0m saver \u001b[38;5;241m=\u001b[39m \u001b[43mPolicySaver\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m saver\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tf_agents/policies/policy_saver.py:333\u001b[0m, in \u001b[0;36mPolicySaver.__init__\u001b[0;34m(self, policy, batch_size, use_nest_path_signatures, seed, train_step, input_fn_and_spec, metadata)\u001b[0m\n\u001b[1;32m    326\u001b[0m get_initial_state_fn\u001b[38;5;241m.\u001b[39mget_concrete_function(\u001b[38;5;241m*\u001b[39mget_initial_state_input_specs)\n\u001b[1;32m    328\u001b[0m train_step_fn \u001b[38;5;241m=\u001b[39m common\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: saved_policy\u001b[38;5;241m.\u001b[39mtrain_step\n\u001b[1;32m    330\u001b[0m )\u001b[38;5;241m.\u001b[39mget_concrete_function()\n\u001b[1;32m    331\u001b[0m get_metadata_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcommon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msaved_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\n\u001b[0;32m--> 333\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_concrete_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m batched_time_step_spec \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m spec: add_batch_dim(spec, [batch_size]), policy\u001b[38;5;241m.\u001b[39mtime_step_spec\n\u001b[1;32m    337\u001b[0m )\n\u001b[1;32m    338\u001b[0m batched_time_step_spec \u001b[38;5;241m=\u001b[39m cast(ts\u001b[38;5;241m.\u001b[39mTimeStep, batched_time_step_spec)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1227\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1226\u001b[0m   \u001b[38;5;66;03m# Implements PolymorphicFunction.get_concrete_function.\u001b[39;00m\n\u001b[0;32m-> 1227\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1228\u001b[0m   concrete\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1197\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1196\u001b[0m     initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1197\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m   1201\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m   \u001b[38;5;66;03m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:695\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[1;32m    691\u001b[0m     variable_capturing_scope,\n\u001b[1;32m    692\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[1;32m    693\u001b[0m )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[0;32m--> 695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[1;32m    700\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[1;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[0;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[1;32m    290\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:331\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    328\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_captures\u001b[38;5;241m.\u001b[39mmerge_by_ref_with(graph_capture_container)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# Create a new FunctionType including captures and outputs.\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m output_type \u001b[38;5;241m=\u001b[39m \u001b[43mtrace_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraced_func_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_context\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m traced_func_type \u001b[38;5;241m=\u001b[39m function_type_lib\u001b[38;5;241m.\u001b[39mFunctionType(\n\u001b[1;32m    335\u001b[0m     function_type\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    336\u001b[0m     traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\u001b[38;5;241m.\u001b[39mcapture_types,\n\u001b[1;32m    337\u001b[0m     return_annotation\u001b[38;5;241m=\u001b[39moutput_type,\n\u001b[1;32m    338\u001b[0m )\n\u001b[1;32m    340\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m concrete_function_lib\u001b[38;5;241m.\u001b[39mConcreteFunction\u001b[38;5;241m.\u001b[39mfrom_func_graph(\n\u001b[1;32m    341\u001b[0m     traced_func_graph,\n\u001b[1;32m    342\u001b[0m     traced_func_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    349\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py:144\u001b[0m, in \u001b[0;36mfrom_value\u001b[0;34m(value, context)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mis_legacy_signature \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, trace\u001b[38;5;241m.\u001b[39mTraceType):\n\u001b[1;32m    143\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, trace\u001b[38;5;241m.\u001b[39mSupportsTracingProtocol):\n\u001b[1;32m    145\u001b[0m   generated_type \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39m__tf_tracing_type__(context)\n\u001b[1;32m    146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generated_type, trace\u001b[38;5;241m.\u001b[39mTraceType):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/typing_extensions.py:689\u001b[0m, in \u001b[0;36m_ProtocolMeta.__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__protocol_attrs__:\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 689\u001b[0m         val \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetattr_static\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/inspect.py:1825\u001b[0m, in \u001b[0;36mgetattr_static\u001b[0;34m(obj, attr, default)\u001b[0m\n\u001b[1;32m   1822\u001b[0m     dict_attr \u001b[38;5;241m=\u001b[39m _shadowed_dict(klass)\n\u001b[1;32m   1823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (dict_attr \u001b[38;5;129;01mis\u001b[39;00m _sentinel \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1824\u001b[0m         \u001b[38;5;28mtype\u001b[39m(dict_attr) \u001b[38;5;129;01mis\u001b[39;00m types\u001b[38;5;241m.\u001b[39mMemberDescriptorType):\n\u001b[0;32m-> 1825\u001b[0m         instance_result \u001b[38;5;241m=\u001b[39m \u001b[43m_check_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1826\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1827\u001b[0m     klass \u001b[38;5;241m=\u001b[39m obj\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/inspect.py:1772\u001b[0m, in \u001b[0;36m_check_instance\u001b[0;34m(obj, attr)\u001b[0m\n\u001b[1;32m   1770\u001b[0m instance_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1771\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1772\u001b[0m     instance_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__dict__\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1773\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m   1774\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: this __dict__ descriptor does not support '_DictWrapper' objects"
     ]
    }
   ],
   "source": [
    "# Save the agent\n",
    "import os\n",
    "from tf_agents.policies.policy_saver import PolicySaver\n",
    "\n",
    "saver = PolicySaver(agent.policy, batch_size=None)\n",
    "saver.save(\"policy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
