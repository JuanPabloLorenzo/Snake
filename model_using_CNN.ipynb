{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scene import Scene\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "\n",
    "scene = Scene(init_randomly=True)\n",
    "N_OUTPUTS = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 5, 5, 12)          444       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 3, 3, 12)          1308      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 108)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                1744      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3564 (13.92 KB)\n",
      "Trainable params: 3564 (13.92 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "        Conv2D(12, (3, 3), activation='relu', padding=\"same\", kernel_initializer='he_normal', input_shape=(scene.height, scene.width, scene.elements_count)),\n",
    "        Conv2D(12, (3, 3), activation='relu', strides=2, padding=\"same\", kernel_initializer='he_normal'),\n",
    "        Flatten(),    \n",
    "        Dense(16, activation='relu', kernel_initializer='he_normal'),\n",
    "        Dense(4, activation='linear')\n",
    "    ])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "target = keras.models.clone_model(model)\n",
    "target.set_weights(model.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 5, 5, 16)          592       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 2, 2, 16)          0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 2, 2, 32)          4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 1, 1, 32)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5828 (22.77 KB)\n",
      "Trainable params: 5828 (22.77 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    Conv2D(16, (3, 3), activation='relu', padding=\"same\", kernel_initializer='he_normal', input_shape=(scene.height, scene.width, scene.elements_count)),\n",
    "    MaxPooling2D(2),\n",
    "    Conv2D(32, (3, 3), activation='relu', padding=\"same\", kernel_initializer='he_normal'),\n",
    "    MaxPooling2D(2),\n",
    "    Flatten(),    \n",
    "    Dense(16, activation='relu', kernel_initializer='he_normal'),\n",
    "    Dense(4, activation='linear')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "target = keras.models.clone_model(model)\n",
    "target.set_weights(model.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(N_OUTPUTS)\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis], verbose=0)\n",
    "        return np.argmax(Q_values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# (state, action, reward, next_state, done)\n",
    "replay_memory = deque(maxlen=20000)\n",
    "\n",
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_memory), size=batch_size)\n",
    "    batch = [replay_memory[index] for index in indices]\n",
    "    states, actions, rewards, next_states, dones = [np.array([experience[field_index] for experience in batch])\n",
    "                                                            for field_index in range(5)]\n",
    "    return states, actions, rewards, next_states, dones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(scene, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done = scene.move(action)\n",
    "    next_state = tf.one_hot(next_state, scene.elements_count)\n",
    "    replay_memory.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_rate = 0.95\n",
    "optimizer = keras.optimizers.legacy.Adam(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "def training_step(batch_size):\n",
    "    states, actions, rewards, next_states, dones = sample_experiences(batch_size)\n",
    "    next_Q_values = target.predict(next_states, verbose=False)\n",
    "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "    target_Q_values = (rewards + (1 - dones) * discount_rate * max_next_Q_values).reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, N_OUTPUTS)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode number:  50\n",
      "Average reward:  0.0\n",
      "Episode number:  100\n",
      "Average reward:  0.0\n",
      "Episode number:  150\n",
      "Average reward:  0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/juanpablo/Desktop/Projects/Snake/model_using_CNN.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_CNN.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     reward_per_batch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_CNN.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m episode \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m50\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_CNN.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     training_step(batch_size)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_CNN.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mif\u001b[39;00m episode \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_CNN.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         target\u001b[39m.\u001b[39mset_weights(model\u001b[39m.\u001b[39mget_weights())\n",
      "\u001b[1;32m/Users/juanpablo/Desktop/Projects/Snake/model_using_CNN.ipynb Cell 8\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_CNN.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_step\u001b[39m(batch_size):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_CNN.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     states, actions, rewards, next_states, dones \u001b[39m=\u001b[39m sample_experiences(batch_size)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_CNN.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     next_Q_values \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39;49mpredict(next_states, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_CNN.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     max_next_Q_values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax(next_Q_values, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juanpablo/Desktop/Projects/Snake/model_using_CNN.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     target_Q_values \u001b[39m=\u001b[39m (rewards \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m dones) \u001b[39m*\u001b[39m discount_rate \u001b[39m*\u001b[39m max_next_Q_values)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/keras/src/engine/training.py:2521\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2512\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m   2513\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   2514\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2515\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2518\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   2519\u001b[0m         )\n\u001b[0;32m-> 2521\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[1;32m   2522\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m   2523\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   2524\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[1;32m   2525\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m   2526\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m   2527\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   2528\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   2529\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   2530\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2531\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution,\n\u001b[1;32m   2532\u001b[0m )\n\u001b[1;32m   2534\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   2535\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1678\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorExactEvalDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1677\u001b[0m     \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 1678\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1304\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1301\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[1;32m   1303\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1304\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step_increment \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mitem() \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1305\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_configure_dataset_and_inferred_steps(\n\u001b[1;32m   1308\u001b[0m     strategy, x, steps_per_epoch, class_weight, distribute\n\u001b[1;32m   1309\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/ops/resource_variable_ops.py:688\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnumpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    687\u001b[0m   \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 688\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_value()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    689\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    690\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/ops/resource_variable_ops.py:818\u001b[0m, in \u001b[0;36mBaseResourceVariable.read_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    815\u001b[0m   value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_variable_op()\n\u001b[1;32m    816\u001b[0m \u001b[39m# Return an identity so it can get placed on whatever device the context\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# specifies instead of the device where the variable is.\u001b[39;00m\n\u001b[0;32m--> 818\u001b[0m \u001b[39mreturn\u001b[39;00m array_ops\u001b[39m.\u001b[39;49midentity(value)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/ops/array_ops.py:303\u001b[0m, in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m    299\u001b[0m   \u001b[39mreturn\u001b[39;00m nest\u001b[39m.\u001b[39mmap_structure(identity, \u001b[39minput\u001b[39m, expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    300\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39minput\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgraph\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    301\u001b[0m   \u001b[39m# Make sure we get an input with handle data attached from resource\u001b[39;00m\n\u001b[1;32m    302\u001b[0m   \u001b[39m# variables. Variables have correct handle data when graph building.\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m   \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    304\u001b[0m ret \u001b[39m=\u001b[39m gen_array_ops\u001b[39m.\u001b[39midentity(\u001b[39minput\u001b[39m, name\u001b[39m=\u001b[39mname)\n\u001b[1;32m    305\u001b[0m \u001b[39m# Propagate handle data for happier shape inference for resource variables.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:1443\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[39m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m preferred_dtype \u001b[39m=\u001b[39m preferred_dtype \u001b[39mor\u001b[39;00m dtype_hint\n\u001b[0;32m-> 1443\u001b[0m \u001b[39mreturn\u001b[39;00m tensor_conversion_registry\u001b[39m.\u001b[39;49mconvert(\n\u001b[1;32m   1444\u001b[0m     value, dtype, name, as_ref, preferred_dtype, accepted_result_types\n\u001b[1;32m   1445\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:209\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    207\u001b[0m overload \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(value, \u001b[39m\"\u001b[39m\u001b[39m__tf_tensor__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    208\u001b[0m \u001b[39mif\u001b[39;00m overload \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m   \u001b[39mreturn\u001b[39;00m overload(dtype, name)  \u001b[39m#  pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39mfor\u001b[39;00m base_type, conversion_func \u001b[39min\u001b[39;00m get(\u001b[39mtype\u001b[39m(value)):\n\u001b[1;32m    212\u001b[0m   \u001b[39m# If dtype is None but preferred_dtype is not None, we try to\u001b[39;00m\n\u001b[1;32m    213\u001b[0m   \u001b[39m# cast to preferred_dtype first.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m   ret \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:1338\u001b[0m, in \u001b[0;36m_EagerTensorBase.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m   1332\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1333\u001b[0m         _add_error_prefix(\n\u001b[1;32m   1334\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAttempting to capture an EagerTensor without \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1335\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mbuilding a function.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1336\u001b[0m             name\u001b[39m=\u001b[39mname))\n\u001b[1;32m   1337\u001b[0m   \u001b[39mreturn\u001b[39;00m graph\u001b[39m.\u001b[39mcapture(\u001b[39mself\u001b[39m, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m-> 1338\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m__tf_tensor__(dtype, name)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:961\u001b[0m, in \u001b[0;36mTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    958\u001b[0m   spec \u001b[39m=\u001b[39m tensor_spec\u001b[39m.\u001b[39mTensorSpec(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, dtype)\n\u001b[1;32m    959\u001b[0m   \u001b[39mreturn\u001b[39;00m spec\n\u001b[0;32m--> 961\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__tf_tensor__\u001b[39m(\n\u001b[1;32m    962\u001b[0m     \u001b[39mself\u001b[39m, dtype: Optional[dtypes\u001b[39m.\u001b[39mDType] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, name: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    963\u001b[0m     ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTensor\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    964\u001b[0m   \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m dtype\u001b[39m.\u001b[39mis_compatible_with(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype):\n\u001b[1;32m    965\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    966\u001b[0m         _add_error_prefix(\n\u001b[1;32m    967\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTensor conversion requested dtype \u001b[39m\u001b[39m{\u001b[39;00mdtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    968\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfor Tensor with dtype \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    969\u001b[0m             name\u001b[39m=\u001b[39mname))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "episodes_count = 1500\n",
    "reward_per_batch = 0\n",
    "\n",
    "for episode in range(1, episodes_count + 1):\n",
    "    #scene.reset()\n",
    "    state = scene.scene_as_matrix()\n",
    "    state = tf.one_hot(state, scene.elements_count)\n",
    "    # aux = episode // 20\n",
    "    # epsilon = 1 - aux / 20\n",
    "    epsilon = 1 / (np.linspace(1, 20, episodes_count))[episode - 1] #(1 / (np.linspace(1, 8, episodes_count)**(1/2)))[episode - 1]\n",
    "    done = False\n",
    "    \n",
    "    steps = 0\n",
    "    while not done and steps < 50:\n",
    "        steps += 1\n",
    "        state, reward, done = play_one_step(scene, state, epsilon)\n",
    "        reward_per_batch += reward\n",
    "        \n",
    "    if episode >= 50:\n",
    "        training_step(batch_size)\n",
    "        if episode % 50 == 0:\n",
    "            target.set_weights(model.get_weights())\n",
    "            reward_per_batch = 0\n",
    "    \n",
    "    if episode % 50 == 0:\n",
    "        print(\"Episode number: \", episode)\n",
    "        print(\"Average reward: \", reward_per_batch / 50)\n",
    "\n",
    "    if (episode + 1) % 500 == 0:\n",
    "        model.save(f\"models/1p_{episode + 1}_episodes.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
